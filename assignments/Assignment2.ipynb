{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/suneman/socialgraphs2017/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Tuesday November 7th at 23:55. Hand in your IPython notebook file (with extension `.ipynb`) via http://peergrade.io/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part I: Comunity Structure\n",
    "\n",
    "We start by looking at the community structure of the the philosopher network. The community detection methods run on an undirected version of the philosopher graph, so create one of those. If your network has more than one component, just work on the _giant connected component_ (GCC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by using the concept of modularity to explore how _community-like_ the six branches of philosophy are. \n",
    "\n",
    "* Explain the concept of modularity in your own words.\n",
    "* Now, *calculate the modularity of the branches reported by the Wikipedia editors*. Modularity is described in the _Network Science_ book, section 9.4). Use **equation 9.12** in the book to calculate the modularity $M$ of the branches-partitioning. \n",
    "\n",
    "> * Hints regarding how to make this happen.\n",
    ">  * Firstly, modularity does not work when the communities are overlapping. Thus, we need to do something about the philosophers that are part of multiple branches. We will handle it by creating a set of _six new branches_, where we take all of the philosophers that belong to more than one branch and assign them to the branch that they have the most connections to. \n",
    "> * Now that we have a new set of non-overlapping branches, we can calculate the modularity\n",
    "\n",
    "* Comment on your value of $M$ for the branches. Are the branches good communities? (We will explore this question in depth below.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, let us use [the Python Louvain-algorithm implementation](http://perso.crans.org/aynaud/communities/) to find communities in the full philosopher network. \\[**Note**: This algorithm is available as Anaconda package. Install with `conda` as expained [here](https://anaconda.org/auto/python-louvain)\\]. \n",
    "\n",
    "Report the value of modularity found by the algorithm. Is it higher or lower than what you found above for the branches as communities? What does this comparison reveal about the branches? \n",
    "\n",
    "> Alternative option: You can also try the *Infomap* algorithm instead if you're curious. Go to [this page]> > (http://www.mapequation.org/code.html) and search for 'python'. It's harder to install, but a better community detection algorithm.\n",
    "\n",
    "* Compare the communities found by your algorithm with the branches (that you analyzed abo ve) by creating a matrix **_D_** with dimension ($B$ times $C$), where $B$ is the number of branches and $C$ is the number of communities. We set entry $D(i,j)$ to be the number of nodes that branch $i$ has in common with community $j$. The matrix **_D_** is what we call a [**confusion matrix**](https://en.wikipedia.org/wiki/Confusion_matrix). Use the confusion matrix to explain how well the communities you've detected correspond to the labeled branches of philosophy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Human navigation paths \n",
    "\n",
    "This exercise works on the wikispeedia dataset. For details on wikispeedia, see [Lecture 8](http://nbviewer.jupyter.org/github/suneman/socialgraphs2016/blob/master/lectures/Week5.ipynb)\n",
    "\n",
    "### IIa: Path lengths\n",
    "\n",
    "The first thing we want to take a look at is path lengths. NetworkX allows us to calculate the shortest path between any pair of articles. We begin by comparing the length of human and shortests paths. \n",
    "\n",
    "* For each _source_/_target_ pair in the list of human navigation paths, calculate the shortest path using NetworkX. Plot the distribution of path lengths. \n",
    "* For each _source_/_target_ pair, calculate the length of the human path. The dataset contains information on people who regret a navigation step and hit the \"back\" button in their web-browser. It's up to you how to incorporate that information in the path. Justify your choice. Plot the distribution of human path lengths. \n",
    "* How much longer are the human paths on average?\n",
    "* Create scatter plot where each point is a _source_/_target_ pair, and you have human path lengths on the $x$-axis and shortests paths on the $y$-axis.\n",
    "* Is there a correlation between human/shortest path-lengths? What is the correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIb: Betweenness\n",
    "\n",
    "An interesting definition of centrality is _betweenness centrality_. In a traditional setting, this measure calculates all shortest paths in the network and then each node gets a score according to which fraction of all shortest paths pass through that node.\n",
    "\n",
    "In this part of the assignment, we create our own version of centrality, based on the _source_/_target_ pairs in our dataset. We define a nodes's **navigation centrality** as follows. \n",
    "\n",
    "> *Navigation centrality* of node $i$ is the fraction of all naviagtion paths that pass through $i$. We exclude the source and target from the count. If a node has not been visited by a search, the navigation centrality of that node is defined to be zero.\n",
    "\n",
    "Below, we investigate the relationship between navigation centrality and betweenness centrality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by calculating the betweenness centrality and navigation centrality of all nodes in the wikispedia dataset.\n",
    "Note that calculating the betweenness centrality can take quite a long time, so you might start it running in a separate notebook while first estimating it based on the existing human path.\n",
    "\n",
    "* First, list the 5 pages with highest navigation centrality.\n",
    "* Second, list the 5 pages with highest betweenness centrality.\n",
    "* Compare the two lists. Explain the differences between the two lists in your own words.\n",
    "* Create a scatterplot of betweenness centrality vs. navigation centrality.\n",
    "* Let's explore the pages that have navigation centrality equal to zero.\n",
    "  * How many pages have zero navigation centrality?\n",
    "  * What is the the page with zero navigation centrality and highest betweenness centrality? Can you explain why no human navigated to this page? Can you explain why the page is central in the actual link network? (For example, you can take a look at the degree of the node).\n",
    "  * Plot the distribution of betweenness centrality for the pages with zero navigation centrality. \n",
    "* Now, let's *throw out all pages with zero navigation centrality* and compare navigation- and betweenness centrality for the remaining pages.\n",
    "  * What is the correlation between betweenness centrality and navigation centrality?\n",
    "  * Comment on the top 5 outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIc: Bringing the text into the picture\n",
    "\n",
    "Now that we have an idea about the differences between how humans and computers search in networks, we are going to dig a little deeper using the page content to test a hypothesis to explain why the human navigation paths are longer. The general idea is that humans (who don't know about the global network structure) tend to jump between pages that have related _content_. For this reason we expect that (on average) human navigation paths have more similar content than the shortest paths in the network (which might take 'surprising' shortcuts via relatively unrelated pages). In short.\n",
    "\n",
    "> **Hypothesis H1**: Human navigation paths have more similar content than network shortest paths.\n",
    "\n",
    "The way we'll test this hypothesis is to first represent each page as a vector using a bag-of-words approach, then we can calculate a distance between pairs of pages using some vector-space difference, and finally we'll characterize each path by its average pair-wise distance. Below, I've set up that process as an exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create a TF-IDF vector for each page based on the ascii version of the page texts. \n",
    "\n",
    "Second, write a function that calculates the distance between a pair of vectors. There are many ways to calculate distances between a pair of vectors (try a Google search for `vector space distance measures` if you want to refresh your knowledge on this topic). You're free to choose what you want, but we recommend the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "\n",
    "Now you're ready to get started\n",
    "\n",
    "* Calculate the average similarity for all human navigation paths (the _source_/_target_ pairs from above). Calculate mean/variance of these average similarities.\n",
    "* Calculate the average similarity for all shortest paths between the _source_/_target_ pairs. Calculate mean/variance of these average similarities.\n",
    "* Plot the distributions of average similarities for both human- and shortest paths in a single plot. If everything works well, you should see something similar to the following:\n",
    "![alt text](https://raw.githubusercontent.com/suneman/socialgraphs2016/master/files/path-similarity.png)\n",
    "* Finally, for each source/target pair, compare the human-navigation average similarity with the betweenness based average similarity, testing what fraction of the time, the average similarity is lower in the case of human navigation.\n",
    "* Comment on your findings. Is **H1** true?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III\n",
    "\n",
    "Exercise, sentiment over some books from NLPP1e\n",
    "\n",
    "* Download the LabMT wordlist. It's available as supplementary material from [Temporal Patterns of Happiness and Information in a Global Social Network: Hedonometrics and Twitter](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0026752) (Data Set S1). Describe briefly how the list was generated.\n",
    "* Based on the LabMT word list, write a function that calculates sentiment given a list of tokens (the tokens should be lower case, etc). The function should complain if there are no words with sentiment attached.\n",
    "* Calculate a sentiment profile for the novels in NLPP1e chapter 1\\. The sentiment profile has sentiment on the $y$-axis and position in the text on the $x$-axis. Use a moving average to show how the sentiment changes. Create profiles for sliding windows of length 50 words, 100 words, 500 words.\n",
    "* Comment on the sentiment profiles. Do they show a similar pattern? What is the effect of changing the size of the sliding window?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
